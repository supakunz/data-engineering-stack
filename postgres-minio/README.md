# Data Engineering Stack

Modern data engineering stack with Apache Airflow, Apache Spark, and MinIO object storage, all containerized with Docker.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Apache        â”‚    â”‚   Apache Spark   â”‚    â”‚     MinIO       â”‚
â”‚   Airflow       â”‚â—„â”€â”€â–ºâ”‚     Cluster      â”‚â—„â”€â”€â–ºâ”‚  Object Storage â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ - Scheduler     â”‚    â”‚ - Master         â”‚    â”‚ - S3 Compatible â”‚
â”‚ - Webserver     â”‚    â”‚ - Worker 1       â”‚    â”‚ - Data Lake     â”‚
â”‚ - Workers       â”‚    â”‚ - Worker 2       â”‚    â”‚ - Buckets       â”‚
â”‚ - Triggerer     â”‚    â”‚                  â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                               â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    PostgreSQL    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚     Database     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚      Redis       â”‚
                        â”‚  Message Broker  â”‚
                        â”‚    (Optional)    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Services

- **Apache Airflow 2.8.0**: Workflow orchestration
- **Apache Spark 3.4.0**: Big data processing
- **MinIO**: S3-compatible object storage
- **PostgreSQL 14**: Metadata database
- **Redis 7.2**: Message broker
- **Jupyter Lab**: Development environment

## ğŸ“‹ Compatible Versions

| Service | Version | Notes |
|---------|---------|--------|
| Airflow | 2.8.0 | Latest stable with Python 3.11 |
| Spark | 3.4.0 | With Hadoop 3.x compatibility |
| MinIO | RELEASE.2023-12-02T10-51-33Z | Latest LTS |
| PostgreSQL | 14 | Airflow metadata database |
| Redis | 7.2-alpine | Celery message broker |
| Python | 3.11 | Runtime for Airflow and Spark |
| Java | 17 | JRE for Spark |

### Key Dependencies

- **PySpark**: 3.4.0 (matches Spark version)
- **Delta Lake**: 2.4.0 (Spark 3.4 compatible)
- **Hadoop AWS**: 3.3.4 (S3A filesystem)
- **AWS Java SDK**: 1.12.367 (S3 connectivity)

## ğŸ› ï¸ Prerequisites

- Docker 20.x or later
- Docker Compose 2.x or later  
- At least 8GB RAM available for Docker
- At least 2 CPU cores

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
git clone <repository>
cd data-engineering-stack

# Make setup script executable
chmod +x setup.sh

# Run setup
./setup.sh
```

### 2. Start Services

```bash
# Start all services in background
docker-compose up -d

# Check service status
docker-compose ps

# View logs
docker-compose logs -f
```

### 3. Access Web Interfaces

| Service | URL | Credentials |
|---------|-----|------------|
| Airflow | http://localhost:8080 | airflow / airflow123 |
| Spark Master | http://localhost:8081 | No auth |
| MinIO Console | http://localhost:9001 | minioadmin / minioadmin |
| Jupyter Lab | http://localhost:8888 | No token |

## ğŸ“ Project Structure

```
data-engineering-stack/
â”œâ”€â”€ docker-compose.yml          # Main orchestration file
â”œâ”€â”€ .env                       # Environment variables
â”œâ”€â”€ setup.sh                   # Setup script
â”‚
â”œâ”€â”€ airflow/                   # Airflow configuration
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ jupyter/                   # Jupyter configuration  
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ spark-defaults.conf
â”‚
â”œâ”€â”€ dags/                      # Airflow DAGs
â”‚   â””â”€â”€ sample_etl_dag.py
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ jobs/                  # Spark applications
â”‚   â”‚   â””â”€â”€ sample_etl_job.py
â”‚   â””â”€â”€ data/                  # Spark data directory
â”‚
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â””â”€â”€ 01_spark_minio_example.ipynb
â”‚
â”œâ”€â”€ logs/                      # Airflow logs
â””â”€â”€ plugins/                   # Airflow plugins
```

## ğŸ”§ Configuration

### MinIO Configuration

Default buckets created automatically:
- `data-lake`: Main data storage
- `spark-logs`: Spark application logs  
- `airflow-logs`: Airflow task logs

### Spark Configuration

Key settings in `spark-defaults.conf`:
- S3A filesystem for MinIO connectivity
- Delta Lake extensions enabled
- Adaptive query execution enabled
- Dynamic allocation configured

### Airflow Configuration

- **Executor**: CeleryExecutor (scalable)
- **Database**: PostgreSQL
- **Broker**: Redis
- **Authentication**: Basic auth enabled

## ğŸ’¡ Usage Examples

### 1. Running the Sample DAG

The included sample DAG demonstrates:
- MinIO connectivity testing
- Sample data creation
- Spark job submission
- Data quality checks

Enable and trigger the DAG:
1. Go to Airflow UI (http://localhost:8080)
2. Find `sample_etl_pipeline` DAG
3. Toggle it ON
4. Click "Trigger DAG"

### 2. Spark Job Development

Create Spark jobs in `spark/jobs/`:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("My ETL Job") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
    .getOrCreate()

# Read from MinIO
df = spark.read.parquet("s3a://data-lake/input/")

# Process data
result = df.groupBy("category").count()

# Write back to MinIO  
result.write.mode("overwrite").parquet("s3a://data-lake/output/")
```

### 3. Using Jupyter for Development

Access Jupyter Lab at http://localhost:8888 and use the sample notebook to:
- Test Spark connectivity
- Experiment with data transformations
- Develop new ETL logic

## ğŸ” Monitoring & Debugging

### Service Health Checks

```bash
# Check all services
docker-compose ps

# Check specific service logs
docker-compose logs -f airflow-webserver
docker-compose logs -f spark-master
docker-compose logs -f minio
```

### Spark Monitoring

- **Spark Master UI**: http://localhost:8081
- **Application History**: Available after jobs complete
- **Worker Logs**: Accessible through Master UI

### Airflow Monitoring

- **Web UI**: http://localhost:8080
- **Task Logs**: Available in UI and `logs/` directory
- **System Health**: Admin â†’ System Health

## ğŸ›¡ï¸ Security Considerations

âš ï¸ **This setup is for development only**

For production:
- Change default passwords
- Enable TLS/SSL
- Configure proper authentication
- Set up network security
- Enable audit logging
- Use secrets management

## ğŸš¨ Troubleshooting

### Common Issues

1. **Port Conflicts**
   ```bash
   # Check port usage
   netstat -tulpn | grep :8080
   
   # Modify ports in docker-compose.yml
   ```

2. **Memory Issues**
   ```bash
   # Reduce memory allocation
   # Edit SPARK_WORKER_MEMORY in docker-compose.yml
   ```

3. **Permission Issues**
   ```bash
   # Fix Airflow permissions
   sudo chown -R $(id -u):$(id -g) logs/ dags/ plugins/
   ```

4. **MinIO Connectivity**
   ```bash
   # Test MinIO connection
   docker-compose exec minio mc admin info minio
   ```

### Reset Everything

```bash
# Stop and remove all containers and volumes
docker-compose down -v

# Remove all images
docker-compose down --rmi all

# Start fresh
./setup.sh
docker-compose up -d
```

## ğŸ“š Additional Resources

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [MinIO Documentation](https://docs.min.io/)
- [Delta Lake Documentation](https://docs.delta.io/)

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Make changes
4. Test thoroughly  
5. Submit pull request

## ğŸ“„ License

This project is licensed under the MIT License.